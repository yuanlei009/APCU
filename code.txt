library(xgboost)
library(data.table)

#step1 Encoding categorical features
dfall <- data.table(apcuclear,keep.rownames = FALSE)
sparse_matrix_all <- sparse.model.matrix(icu ~.,data = dfall)[,-1]

df <- data.table(trainset,keep.rownames = FALSE)
head(df)
str(df)
sparse_matrix <- sparse.model.matrix(icu ~.,data = df)[,-1]
head(sparse_matrix)
output_vector = data.matrix(df[,icu])

#step2 find the best training parameters
grid = expand.grid(
  nrounds = c(20,25,30,35),
  colsample_bytree = 1,
  min_child_weight = 1,
  eta = c(0.1, 0.2, 0.3,0.4), #0.3 is default,
  gamma = c(0.1, 0.2,0.3,0.4),
  subsample = c(0.2,0.3,0.4,0.5),
  max_depth = c(2,3,4,5,6,8)
)

cntrl = trainControl(
  method = "cv",
  number = 5,
  verboseIter = TRUE,
  returnData = FALSE,
  returnResamp = "final"
)
train.xgb = train(
  x = sparse_matrix,
  y = output_vector,
  trControl = cntrl,
  tuneGrid = grid,
  method = "xgbTree"
)


#step3 train the xgboost model using the best training parameters
param <- list(
              max_depth = 5,
              eta = 0.3, 
              gamma = 0.3,
              colsample_bytree = 1,
              min_child_weight = 1,
              subsample = 0.5,
              eval_metric = "auc",
              silent = 1,
              booster = 'gbtree'
              )

model <- xgboost(
                   param,
                   data = sparse_matrix,
                   label = output_vector,
                   nrounds = 35,
                   objective='binary:logistic'
                   )
